Κωνσταντίνος
Αργυρόπουλος
1115201500012
Προγραμματισμός Συστήματος-Εργασία 3η

# webcreator.sh

Η εφαρμογή webcreator ελέγχει αρχικά ότι υπάρχουν το textfile από το οποίο δέχεται την είσοδο και ότι έχει τουλάχιστον 10000 γραμμές,το directory στο οποίο θα αποθηκευτούν τα αποτελέσματα, καθώς και ότι τα w,p είναι ακέραιοι αριθμοί.Να σημειώσουμε ότι στο παραδοτέο,το directory θα πρέπει να είναι στον ίδιο φάκελο με την εφαρμογή αυτή,για λόγους ταχύτητας,ωστόσο θα μπορούσαμε να το βρούμε και με την εντολή
/* find / -name "$1" -type d -print */ που θα σάρωνε όλα τα αρχεία μέχρι να βρει το root_directory.
Αν το root_directory είναι ήδη γραμμένο (πιθανόν από προηγούμενη εκτέλεση) τότε διαγράφουμε ότι υπάρχει μέσα σε αυτό και εκτυπώνεται κατάλληλο μύνημα.
Στη συνέχεια δημιουργούνται 2 πίνακες που κρατούν τα ονόματα της κάθε σελίδας αλλά και τα αντίστοιχα links.
Δημιουργούνται οι κατάλογοι-sites για κάθε page καλείται η συνάρτηση pageContent,η οποία δίνει περιεχόμενο στα pages.
Τέλος, με τη βοήθεια του πίνακα writtenLinks,ο οποίος παίρνει ένα νέο στοιχείο όταν βρίσκει κάποιο link για πρώτη φορά,ελέγχουμε αν όλες οι σελίδες έχουν τουλάχιστον έναν εισερχόμενο σύνδεσμο και εκτυπώνεται κατάλληλο μύνημα.

Συνάρτηση pageContent
Αρχικά δίνουμε τυχαίες τιμές στα επιθυμητά όρια για τα k,m και αρχικοποιούμε τα f,q.
Στη συνέχεια δημιουργείται ο πίνακας lflag,ο οποίος κρατάει τον αριθμό του κάθε site,ώστε να ελέγχεται αν ένα link είναι εσωτερικό ή εξωτερικό.
Η βασική επανάληψη στην οποία βρίσκουμε ποιά links θα γραφούν μέσα στη σελίδα,έχει τρεις λογικές μεταβλητές flag,exflag,inflag και τους μετρητές incount και excount που μετρούν το πόσα και τι είδους links έχουν γραφτεί,ενώ όλα τα προς εγγραφή links παιρνούν στον linksToBeWritter.Για να μην γίνει εισαγωγή του ίδιου link περισσότερες από 1 φορές το προς εγγραφή link γίνεται unset από τον πίνακα των links,με τη βοήθεια της συνάρτησης unset.Τέλος γίνονται οι κατάλληλες εκτυπώσεις και γράφουμε τη σελίδα.

Να τονιστεί ότι οι δοκιμές των server και crawler έγιναν με telnet.

# web server

Η εφαρμογή του web server αποτελείται από 2 αρχεία,το myhttpd.c και το httpd_threads.c .
Η εντολή μεταγλώττισης είναι make,τα αρχεία βρίσκονται στο φάκελο server μαζί με το αρχείο myhttpd.h και το Makefile.

myhttpd.c
Στο myhttpd.c βρίσκονται η main και ορισμένες βοηθητικές συναρτήσεις.
Αρχικά εισάγονται τα ορίσματα από τη γραμμή εντολής,μαζί με έναν μικρό έλεγχο εγκυρότητας.Αρχικοποιούνται τα mutexes,condition variables και ότι εξωτερικές μεταβλητές θα χρειαστούμε καθώς και η βασική δομή-τhreadpool με το όνομα stats,με τη συνάρτηση statsInit.
Δημιουργούνται τα threads,που λειτουργούν στη συνάρτηση ThreadTask και στη συνέχεια ξεκινά η χρονομέτρηση.
Η λειτουργία των threads θα εξηγηθεί στη συνέχεια.
Δημιουργούνται 2 συνδέσεις socket,μία στο command και μία serving port και τα file descriptors των συνδέσεων μπαίνουν σε ένα πίνακα,τον fds.
Ο server τερματίζει όταν η εξωτερική λογική μεταβλητή sthutdownTime γίνει 1.
Αρχικά, καλείται η συνάρτηση findPort που επιστρέφει τον αριθμό του port στο οποίο έχει γίνει η σύνδεση.
Λειτουργία findPort
Αφότου αρχικοποιηθούν οι μεταβλητές καλείται η select,η οποία περιμένει μέχρι να γίνει σύνδεση και επιστρέφει στη μεταβλητή readfds ένα αναγνωριστικό για το port που έχει επιλεχθεί.Ελέγχεται ποιό από τα δύο port έχει οριστεί στη readfds και επιστρέφει τον αναγνωριστικό του αριθμό.
Εάν πρόκειται για serving_socket,τότε γίνει εισαγωγή του client_socket στην ουρά και ενεργοποιούνται τα threads.
Αλλιώς,διαβάζει το request ελέγχει τι είδους είναι--stats/shutdown και είτε επιστρέφει μέσω της ήδη ανοιχτής socket τα αποτελέσματα,είτε κλείνει τις σύνδεσεις,τερματίζει τα threads και τερματίζει.

httpd_threads.c
Στο αρχείο αυτό,βρίσκονται η βασική συνάρτηση λειτουργίας των νημάτων,καθώς και βοηθητικές συναρτήσεις για την εύρεση της απάντησης στα αιτήματα από τον server.
threadTask
Τα threads λειτουργούν ως εξής.Αρχικά,ενημερώνουν τη main ότι είναι όλα έτοιμα,ώστε αυτή να συνεχίσει.Στη συνέχεια μπλοκάρουν μέχρι η main να τα ενημερώσει ότι υπάρχει νέο αίτημα που πρέπει να επεξεργαστούν.Κάθε thread παίρνει--αν υπάρχει-- ένα fd από την ουρά,διαβάζει το αίτημα,βρίσκει την απάντηση σε αυτό και τη γράφει στο fd που διαχειρίζεται.Στη συνέχεια ενημερώνει τη main ότι η δουλεία του ολοκληρώθηκε και περιμένει σήμα από αυτή ώστε να επαναληφθεί η παραπάνω διαδικασία.
Η απόκριση στο αίτημα,γινεται μέσω της getResponce,η οποία επίσης βρίσκει τον αριθμό των σελίδων που έχουν επιστραφεί και των bytes που έχουν γραφτεί,ως απάντηση στο STATS.
Η getContent βρίσκει το περιεχόμενο της απάντησης που θα επιστρέψει ο server,ελέγχοντας επίσης αν υπάρχει απάντηση στο αίτημα και αν τα δικαιώματα είναι σωστά.

# web mycrawler

Η εφαρμογή του web crawler αποτελείται από 2 αρχεία,το mycrawler.c και το crawler_threads.c .
Επίσης έχουν συμπεριληφθεί τα αρχεία connections.c , jobExecutor.c , trie.c searchMode.c ,με μια συνάρτηση να έχει προστεθεί σε σχέση με την Εργασία 2,που έχει να κάνει με το πέρασμα των search queries από τον crawler στη jobExecutor
Η εντολή μεταγλώττισης είναι make,τα αρχεία βρίσκονται στο φάκελο client μαζί με το αρχείο mycrawler.h και το Makefile.

mycrawler.c
Στο αρχείο αυτό,βρίσκονται η main και ορισμένες βοηθητικές συναρτήσεις.
Αρχικά εισάγονται τα ορίσματα από τη γραμμή εντολής,μαζί με έναν μικρό έλεγχο εγκυρότητας.Αρχικοποιούνται τα mutexes,condition variables και ότι εξωτερικές μεταβλητές θα χρειαστούμε καθώς και η βασική δομή-τhreadpool με το όνομα thread_pool,με τη συνάρτηση poolInit.
Η ουρά εργασίας με την οποία λειτουργούν τα νήματα, έχει ως κλειδιά τα urls των αρχείων ως προς εξερεύνηση.
Δημιουργούνται τα threads,που λειτουργούν στη συνάρτηση threadTask και ελέγχεται από τη main,μέ τη βοήθεια των mutexes,condition variables και ορισμένων εξωτερικών μεταβλητών,ότι τα threads είναι έτοιμα.Θα πούμε περισσότερα σε λίγο.
Στη συνέχεια ξεκινά η χρονομέτρηση.Δημιουργείται η socket στο command_port και ο crawler περιμένει για συνδέσεις.
Αν δοθεί η εντολή search,τότε (και εφόσον έχει ολοκληρωθεί το crawling) καλείται η addLinksToFile,η οποία με τη βοήθεια της δομής paths έχει σε μια συνδεδεμένη λίστα τα μονοπάτια για όλα τα αρχεία που έχει κάνει crawling και τα περνάει στο αρχείο που δημιουργείται με όνομα paths.txt,το οποίο λειτουργεί σαν όρισμα στη jobExecutor.Επίσης περνάει το fd της socket και τα search queries.
Οι αλλαγές της jobExecutor είναι ότι πλέον by default λειτουργούν 5 workers και η συνάρτηση findEachQuery που επιστρέφει τον αριθμό των search queries.Επίσης,δεν υπάρχει deadline και δεν γράφονται τα στατιστικά σε logFile.

crawler_threads.c
Στο αρχείο αυτό,βρίσκονται η βασική συνάρτηση λειτουργίας των νημάτων,καθώς και βοηθητικές συναρτήσεις για τη σωστή λειτουργία του crawler.
threadTask
Τα threads λειτουργούν ως εξής.Αρχικά,ενημερώνουν τη main ότι είναι όλα έτοιμα,ώστε αυτή να συνεχίσει και να τους στείλει σήμα να ξεκινήσουν.
Μια εξωτερική μεταβλητή blocked αυξάνεται κάθε φορά που η dequeue επιστρέφει null.
Όταν η blocked γίνει ίση με των αριθμό των νημάτων,σημαίνει ότι όλα τα νήματα έχουν μπλοκάρει και επομένως η διαδικασία έχει ολοκληρωθεί.
Μια εξωτερική μεταβλητή unblocked κάνει τα νήματα να περιμένουν.Η μεταβλητή αυτή γίνεται 1,όταν κάποιο thread ολοκληρώσει τη διαδικασία του οπότε και θα έχει βάλει καινούργια στοιχεία προς επεξεργασία στην ουρά.
Μια εξωτερικλη μεταβλητή work_done αυξάνεται κάθε φορά που τερματίζει κάποιο thread,ώστε να γίνεται γνωστό ότι το crawling έχει ολοκληρωθεί--απαραίτητο για να γίνει η SEARCH.
contentProcess
Ο crawler σε μορφή client δημιουργεί socket connection με τον server,στον οποίο στέλνει αίτημα για κάποιο url που βρίσκεται στην ουρά και διαβάζει την απάντηση μέσω της appendFile.Αυτή ανοίγει ή δημιοργεί το κατάλληλο directory-site στο save_dir-αν πρόκειται για δημιουργεία νέου directory το string αυτό γράφεται και στη λίστα με τα paths-,γράφει σε αυτό την απάντηση του server και κλείνει το αρχείο.Στη συνέχεια καλείται η συνάρτηση analyzeIt,η οποία ανοίγει το αρχείο βλέπει τα περιεχόμενά του και περνάει τα links του στην ουρά εργασίας.Ωστόσο έτσι περνούνται στην ουρά links που έχουν ήδη αναλυθεί με τον τρόπο αυτό.Επομένως στην threadTask αρχικά γίνεται έλεγχος αν το link υπάρχει σε μια λίστα με όλα τα links που έχουν αναλυθεί.Αν υπάρχει,τότε συνεχίζουν τα threads με το επόμενο στοιχείο της ουράς,αλλιώς εισάγεται το στοιχείο αυτό.
